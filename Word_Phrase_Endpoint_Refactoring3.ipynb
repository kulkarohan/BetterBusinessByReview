{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word_Phrase_Endpoint_Refactoring3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LilySu/BetterBusinessByReview/blob/master/Word_Phrase_Endpoint_Refactoring3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKzD21T4acWg",
        "colab_type": "code",
        "outputId": "f03050c4-cca5-4d1c-83c3-ea281a202bbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "#breaks reviews up into individual words, tallies up word occurrences and extracts phrases where word appears.\n",
        "#spacy and scattertext are not used because the results are decent without it and for companies with few reviews, compute time is instant.\n",
        "\n",
        "\n",
        "import json\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from lxml import html\n",
        "from requests import Session\n",
        "from concurrent.futures import ThreadPoolExecutor as Executor\n",
        "import requests\n",
        "import re\n",
        "pd.options.display.max_rows = 999\n",
        "pd.options.display.max_columns = 999\n",
        "pd.set_option('display.max_colwidth', 1000)\n",
        "base_url = \"https://www.yelp.com/biz/\" \n",
        "api_url = \"/review_feed?sort_by=date_desc&start=\"\n",
        "bid = 'EsIHcOL5XI_W--no5xt_wQ'\n",
        "\n",
        "\n",
        "class Scraper():\n",
        "    def __init__(self):\n",
        "        self.data = pd.DataFrame()\n",
        "\n",
        "    def get_data(self, n, bid=bid):\n",
        "        with Session() as s:\n",
        "            with s.get(base_url+bid+api_url+str(n*20)) as resp: #makes an http get request to given url and returns response as json\n",
        "                r = json.loads(resp.content) #converts json response into a dictionary\n",
        "                _html = html.fromstring(r['review_list']) #loads from dictionary\n",
        "\n",
        "                dates = _html.xpath(\"//div[@class='review-content']/descendant::span[@class='rating-qualifier']/text()\")\n",
        "                reviews = [el.text for el in _html.xpath(\"//div[@class='review-content']/p\")]\n",
        "                ratings = _html.xpath(\"//div[@class='review-content']/descendant::div[@class='biz-rating__stars']/div/@title\")\n",
        "\n",
        "                df = pd.DataFrame([dates, reviews, ratings]).T\n",
        "\n",
        "                self.data = pd.concat([self.data,df])\n",
        "\n",
        "    def scrape(self): #makes it faster\n",
        "        # multithreaded looping\n",
        "        with Executor(max_workers=40) as e:\n",
        "            list(e.map(self.get_data, range(10)))\n",
        "\n",
        "s = Scraper()\n",
        "s.scrape()\n",
        "df = s.data\n",
        "df = df.dropna()\n",
        "\n",
        "df['word_segments_unpacked'] = df[1].apply(lambda x: x[1:-1].split(' '))#turn string comma separated list per word\n",
        "\n",
        "df['word_segments_unpacked'] = df['word_segments_unpacked'].astype(str)\n",
        "df['word_segments_unpacked'] = df['word_segments_unpacked'].apply(lambda x: ''.join([str(i) for i in x]))\n",
        "phrase_count = df[['word_segments_unpacked', 2]]\n",
        "\n",
        "\n",
        "s= phrase_count.apply(lambda x: pd.Series(x['word_segments_unpacked']),axis=1).stack().reset_index(level=1, drop=True)\n",
        "s.name = 'word_segments_unpacked'\n",
        "\n",
        "phrase_count = phrase_count.drop('word_segments_unpacked', axis=1).join(s)\n",
        "phrase_count = pd.DataFrame(df['word_segments_unpacked'].str.split(',').tolist(), index=df[2]).stack()\n",
        "\n",
        "phrase_count = phrase_count.reset_index()[[0, 2]] # var1 variable is currently labeled 0\n",
        "phrase_count.columns = ['word_segments_unpacked', 'ratings'] # renaming var1\n",
        "phrase_count = phrase_count.reset_index(drop=False)\n",
        "replace_dict_phrase_count = {'[':'',']':'','-':'','!':'','.':'',' ':'',\"'\":''}\n",
        "for key in replace_dict_phrase_count.keys():\n",
        "  phrase_count['word_segments_unpacked'] = phrase_count['word_segments_unpacked'].str.replace(key, replace_dict_phrase_count[key])\n",
        "phrase_count['word_segments_unpacked'] = phrase_count['word_segments_unpacked'].str.lower()\n",
        "\n",
        "stopwords = [')','(','\\(','\\xa0','0','1','2','3','4','5','6','7','8','9','/','$',\"'d\",\"'ll\",\"'m\",'+','maybe','from','first','here','only','put','where','got','sure','definitely','food','yet','our','go','since','really','very','two',\"n't\",'with','if',\"'s\",'which','came','all','me','(',')','makes','make','were','immediately','get','been','ahead','also','that','one','have','see','what','to','we','had','.',\"'re\",'it','or','he','she','we','us','how','went','no','\"','of','has','by','bit','thing','place','so','ok','and','they','none','was','you',\"'ve\",'did','be','and','but','is','as','&','you','has','-',':','and','had','was','him','so','my','did','would','her','him','it','is','by','bit','thing','place','[',']','while','check-in','=','= =','want', 'good','husband', 'want','love','something','your','they','your','cuz','him',\"i've\",'her','told', 'check', 'i\"m', \"it's\",'they', 'this','its','they','this',\"don't\",'the',',', 'it', 'i\"ve', 'i\"m', '!', '1','2','3','4', '5','6','7','8','9','0','/','.']\n",
        "def filter_stopwords(text):\n",
        "  for i in str(text):\n",
        "    if i not in stopwords:\n",
        "      return str(text)\n",
        "\n",
        "#if item in stopwords list partially matches, delete, single letters like 'i' would be deleted from inside individual words if in list\n",
        "phrase_count = phrase_count[~phrase_count['word_segments_unpacked'].isin(stopwords)]\n",
        "#if the following words fully matches, filter out\n",
        "full_match_list = ['i','a','an','am','at','are','in','on','for','','\\xa0\\xa0','\\xa0','\\(']\n",
        "phrase_count = phrase_count[~phrase_count['word_segments_unpacked'].isin(full_match_list)]\n",
        "\n",
        "#pivot table ratings\n",
        "phrase_count_pivot = pd.pivot_table(phrase_count, index='word_segments_unpacked', columns='ratings', aggfunc='count', fill_value=0)\n",
        "phrase_count_pivot.columns = [''.join(col).strip() for col in phrase_count_pivot.columns.values]#flatten index levels part 1\n",
        "phrase_count_pivot = pd.DataFrame(phrase_count_pivot.to_records())#flatten index levels part 2\n",
        "\n",
        "#if there are no _# star reviews, add a column of zeros\n",
        "required_column_names = ['index1.0 star rating', 'index2.0 star rating','index3.0 star rating','index4.0 star rating','index5.0 star rating']\n",
        "for i in required_column_names:\n",
        "  if i not in phrase_count_pivot.columns:\n",
        "    phrase_count_pivot[i] = 0\n",
        "phrase_count_pivot.sample(10)\n",
        "\n",
        "#replace the original count by getting an exaggerated scaled tally of reviews to calculate score\n",
        "phrase_count_pivot['index1.0 star rating'] = phrase_count_pivot['index1.0 star rating']*(-2)\n",
        "phrase_count_pivot['index2.0 star rating'] = phrase_count_pivot['index2.0 star rating']*(-1)\n",
        "phrase_count_pivot['index3.0 star rating'] = phrase_count_pivot['index3.0 star rating']*(-0.1)\n",
        "phrase_count_pivot['index4.0 star rating'] = phrase_count_pivot['index4.0 star rating']*(1)\n",
        "phrase_count_pivot['index5.0 star rating'] = phrase_count_pivot['index5.0 star rating']*(2)\n",
        "\n",
        "#get a total score from the sum of exaggerated scores\n",
        "phrase_count_pivot['score'] = phrase_count_pivot['index1.0 star rating'] + phrase_count_pivot['index2.0 star rating'] + phrase_count_pivot['index3.0 star rating'] + phrase_count_pivot['index4.0 star rating'] + phrase_count_pivot['index5.0 star rating']\n",
        "\n",
        "phrase_count_pivot['score'] = phrase_count_pivot['score'].div(phrase_count_pivot['score'].max(), axis=0)#normalize\n",
        "phrase_count_pivot['score'] = phrase_count_pivot['score'].round(decimals=4)#round to 4 decimal places\n",
        "phrase_count_pivot = phrase_count_pivot.sort_values(by=('score'), ascending=False)\n",
        "phrase_count_pivot.head(2)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_segments_unpacked</th>\n",
              "      <th>index5.0 star rating</th>\n",
              "      <th>index1.0 star rating</th>\n",
              "      <th>index2.0 star rating</th>\n",
              "      <th>index3.0 star rating</th>\n",
              "      <th>index4.0 star rating</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>move</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>ed</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.7778</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    word_segments_unpacked  index5.0 star rating  index1.0 star rating  \\\n",
              "143                   move                    18                     0   \n",
              "73                      ed                    14                     0   \n",
              "\n",
              "     index2.0 star rating  index3.0 star rating  index4.0 star rating   score  \n",
              "143                     0                  -0.0                     0  1.0000  \n",
              "73                      0                  -0.0                     0  0.7778  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r-14Zh1YQMpR",
        "colab": {}
      },
      "source": [
        "phrase_count_pivot['word_segments_unpacked'] = phrase_count_pivot['word_segments_unpacked'].str.replace('\\(', '')\n",
        "phrase_count_pivot['word_segments_unpacked'] = phrase_count_pivot['word_segments_unpacked'].str.replace('(', '')\n",
        "phrase_count_pivot['word_segments_unpacked'] = phrase_count_pivot['word_segments_unpacked'].str.replace(')', '')#without these, errors incurr\n",
        "\n",
        "worst_terms_list = [] \n",
        "top_terms_list = []\n",
        "x,y = phrase_count_pivot.shape#tuple unpacking to get the length of the dataframe\n",
        "for i in reversed(range(x)):\n",
        "  try:\n",
        "    new_df = df[df[1].str.contains(phrase_count_pivot['word_segments_unpacked'].iloc[i])]#if word appears in review, create a dataframe with each row being the word occurring in a different review\n",
        "    neg_first_df = new_df.sort_values(by=2, ascending=True)#rank the dataframe with worst reviews first\n",
        "    pos_first_df = new_df.sort_values(by=2, ascending=False)#rank the dataframe with most positive reviews first\n",
        "    if neg_first_df[1].iloc[0] not in worst_terms_list:#get the lowest star rating review\n",
        "      worst_terms_list.append(neg_first_df[1].iloc[0])#prevent duplicates\n",
        "    if pos_first_df[1].iloc[0] not in top_terms_list:#get the highest star rating review\n",
        "      top_terms_list.append(pos_first_df[1].iloc[0])\n",
        "  except IndexError as e:\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMc4NAWQavn5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "negative_list = []\n",
        "for i in range(-30,0):#take the worst 30 terms\n",
        "  for list_of_words in worst_terms_list:\n",
        "    word_list = list_of_words.split(' ')\n",
        "    for word in word_list:\n",
        "      word = word.replace(',','')\n",
        "      word = word.replace('.','')\n",
        "      try: \n",
        "        if phrase_count_pivot['word_segments_unpacked'].iloc[i] == word: #find word occurrence in original comma separated word list of reviews\n",
        "          try:\n",
        "            index = word_list.index(word)\n",
        "            string_from_phrases = ','.join(word_list[max(0,index-5):min(index+20, len(word_list))])\n",
        "            replace_dict_string_from_phrases= {'\\xa0':'',' ':'',',':' ',' .':'.','!':'','[':'',']':'','\\n':'','1':'','2':'','3':'','4':'','5':'','6':'','7':'','8':'','9':'','0':'','/':'',\"'\":\"'\",\"'\":''}\n",
        "            for key in replace_dict_string_from_phrases.keys():\n",
        "              string_from_phrases=string_from_phrases.replace(key, replace_dict_string_from_phrases[key])\n",
        "            negative_list.append(string_from_phrases)\n",
        "          except ValueError as e:\n",
        "            pass\n",
        "      except IndexError as e:#if there are less than 30 words after stopword filtering, just get the first word and its occurrence in the original review\n",
        "        if phrase_count_pivot['word_segments_unpacked'].iloc[0] == word:\n",
        "          try:\n",
        "            index = word_list.index(word)\n",
        "            string_from_phrases = ','.join(word_list[max(0,index-5):min(index+20, len(word_list))])\n",
        "            replace_dict_string_from_phrases= {'\\xa0':'',' ':'',',':' ',' .':'.','!':'','[':'',']':'','\\n':'','1':'','2':'','3':'','4':'','5':'','6':'','7':'','8':'','9':'','0':'','/':'',\"'\":\"'\",\"'\":''}\n",
        "            for key in replace_dict_string_from_phrases.keys():\n",
        "              string_from_phrases=string_from_phrases.replace(key, replace_dict_string_from_phrases[key])\n",
        "            negative_list.append(string_from_phrases)\n",
        "          except ValueError as e:\n",
        "            pass\n",
        "negative_df = pd.DataFrame(negative_list)\n",
        "negative_df = negative_df.reset_index(drop=False)\n",
        "negative_df = negative_df.rename(columns={'index':'score', 0 : 'term'})\n",
        "negative_df = negative_df.drop_duplicates(subset='term')\n",
        "x,y = negative_df.shape#tuple unpacking to get the length of the dataframe\n",
        "if x < 10:\n",
        "  for i in range(-40,-30):\n",
        "    for list_of_words in worst_terms_list:\n",
        "      word_list = list_of_words.split(' ')\n",
        "      for word in word_list:\n",
        "        word = word.replace(',','')\n",
        "        word = word.replace('.','')\n",
        "        try:\n",
        "          if phrase_count_pivot['word_segments_unpacked'].iloc[i] == word:\n",
        "            try:\n",
        "              index = word_list.index(word)\n",
        "              string_from_phrases = ','.join(word_list[max(0,index-5):min(index+20, len(word_list))])\n",
        "              replace_dict = {'\\xa0':'',' ':'',',':' ',' .':'.','!':'','[':'',']':'','\\n':'','1':'','2':'','3':'','4':'','5':'','6':'','7':'','8':'','9':'','0':'','/':'',\"'\":\"'\",\"'\":''}\n",
        "              for key in replace_dict.keys():\n",
        "                string_from_phrases=string_from_phrases.replace(key, replace_dict[key])\n",
        "              negative_list.append(string_from_phrases)\n",
        "            except ValueError as e:\n",
        "              pass\n",
        "        except IndexError as e:\n",
        "          if phrase_count_pivot['word_segments_unpacked'].iloc[0] == word:\n",
        "            try:\n",
        "              index = word_list.index(word)\n",
        "              string_from_phrases = ','.join(word_list[max(0,index-5):min(index+20, len(word_list))])\n",
        "              replace_dict_string_from_phrases= {'\\xa0':'',' ':'',',':' ',' .':'.','!':'','[':'',']':'','\\n':'','1':'','2':'','3':'','4':'','5':'','6':'','7':'','8':'','9':'','0':'','/':'',\"'\":\"'\",\"'\":''}\n",
        "              for key in replace_dict_string_from_phrases.keys():\n",
        "                string_from_phrases=string_from_phrases.replace(key, replace_dict_string_from_phrases[key])\n",
        "              negative_list.append(string_from_phrases)\n",
        "            except ValueError as e:\n",
        "              pass\n",
        "negative_df_addon = pd.DataFrame(negative_list)\n",
        "negative_df_addon = negative_df_addon.reset_index(drop=False)\n",
        "negative_df_addon = negative_df_addon.rename(columns={'index':'score', 0 : 'term'})\n",
        "negative_df = pd.concat([negative_df, negative_df_addon])\n",
        "negative_df = negative_df.head(10)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cl1Aqp7ta0j_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_list = []\n",
        "for i in range(0,30):\n",
        "  for list_of_words in top_terms_list:\n",
        "    word_list = list_of_words.split(' ')\n",
        "    for word in word_list:\n",
        "      word = word.replace(',','')\n",
        "      word = word.replace('.','')\n",
        "      try: \n",
        "        if phrase_count_pivot['word_segments_unpacked'].iloc[i] == word:\n",
        "          try:\n",
        "            index = word_list.index(word)\n",
        "            string_from_phrases = ','.join(word_list[max(0,index-5):min(index+20, len(word_list))])\n",
        "            replace_dict = {'\\xa0':'',' ':'',',':' ',' .':'.','!':'','[':'',']':'','\\n':'','1':'','2':'','3':'','4':'','5':'','6':'','7':'','8':'','9':'','0':'','/':'',\"'\":\"'\",\"'\":''}\n",
        "            for key in replace_dict.keys():\n",
        "              string_from_phrases=string_from_phrases.replace(key, replace_dict[key])\n",
        "            negative_list.append(string_from_phrases)\n",
        "          except ValueError as e:\n",
        "            pass\n",
        "      except IndexError as e:\n",
        "        if phrase_count_pivot['word_segments_unpacked'].iloc[0] == word:\n",
        "          try:\n",
        "            index = word_list.index(word)\n",
        "            string_from_phrases = ','.join(word_list[max(0,index-5):min(index+20, len(word_list))])\n",
        "            replace_dict = {'\\xa0':'',' ':'',',':' ',' .':'.','!':'','[':'',']':'','\\n':'','1':'','2':'','3':'','4':'','5':'','6':'','7':'','8':'','9':'','0':'','/':'',\"'\":\"'\",\"'\":''}\n",
        "            for key in replace_dict.keys():\n",
        "              string_from_phrases=string_from_phrases.replace(key, replace_dict[key])\n",
        "            negative_list.append(string_from_phrases)\n",
        "          except ValueError as e:\n",
        "            pass\n",
        "  positive_list.append(string_from_phrases)\n",
        "positive_df = pd.DataFrame(positive_list)\n",
        "positive_df = positive_df.reset_index(drop=False)\n",
        "positive_df = positive_df.rename(columns={'index':'score', 0 : 'term'})\n",
        "positive_df = positive_df.drop_duplicates(subset='term')\n",
        "x,y = positive_df.shape#tuple unpacking to get the length of the dataframe\n",
        "for i in range(30,40):\n",
        "  for list_of_words in top_terms_list:\n",
        "    word_list = list_of_words.split(' ')\n",
        "    for word in word_list:\n",
        "      word = word.replace(',','')\n",
        "      word = word.replace('.','')\n",
        "      try:\n",
        "        if phrase_count_pivot['word_segments_unpacked'].iloc[i] == word:\n",
        "          try:\n",
        "            index = word_list.index(word)\n",
        "            string_from_phrases = ','.join(word_list[max(0,index-5):min(index+20, len(word_list))])\n",
        "            replace_dict = {'\\xa0':'',' ':'',',':' ',' .':'.','!':'','[':'',']':'','\\n':'','1':'','2':'','3':'','4':'','5':'','6':'','7':'','8':'','9':'','0':'','/':'',\"'\":\"'\",\"'\":''}\n",
        "            for key in replace_dict.keys():\n",
        "              string_from_phrases=string_from_phrases.replace(key, replace_dict[key])\n",
        "            negative_list.append(string_from_phrases)\n",
        "          except ValueError as e:\n",
        "            pass\n",
        "      except IndexError as e:\n",
        "        if phrase_count_pivot['word_segments_unpacked'].iloc[0] == word:\n",
        "          try:\n",
        "            index = word_list.index(word)\n",
        "            string_from_phrases = ','.join(word_list[max(0,index-5):min(index+20, len(word_list))])\n",
        "            replace_dict = {'\\xa0':'',' ':'',',':' ',' .':'.','!':'','[':'',']':'','\\n':'','1':'','2':'','3':'','4':'','5':'','6':'','7':'','8':'','9':'','0':'','/':'',\"'\":\"'\",\"'\":''}\n",
        "            for key in replace_dict.keys():\n",
        "              string_from_phrases=string_from_phrases.replace(key, replace_dict[key])\n",
        "            negative_list.append(string_from_phrases)\n",
        "          except ValueError as e:\n",
        "            pass\n",
        "positive_df_addon = pd.DataFrame(negative_list)\n",
        "positive_df_addon = positive_df_addon.reset_index(drop=False)\n",
        "positive_df_addon = positive_df_addon.rename(columns={'index':'score', 0 : 'term'})\n",
        "positive_df = pd.concat([positive_df, positive_df_addon])\n",
        "positive_df = positive_df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdxKAvi8M-2J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "outputId": "def7a288-e903-4815-ddb1-d3698c1fe854"
      },
      "source": [
        "results = {'positive': [{'term': pos_term, 'score': pos_score} for pos_term, pos_score in zip(positive_df['term'], positive_df['score'])], 'negative': [{'term': neg_term, 'score': neg_score} for neg_term, neg_score in zip(negative_df['term'], negative_df['score'])]}\n",
        "results"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'negative': [{'score': 0,\n",
              "   'term': 'furniture and also moved many loose ends that we quickly threw in open boxes. They even took our three dressers  packed with clothes  as is '},\n",
              "  {'score': 1,\n",
              "   'term': 'impossible feat and managed to lower my  foot tall glass showcase (which wouldnt have fit down my very weirdly laid out nd floor staircase)'},\n",
              "  {'score': 2, 'term': 'Ed and Norris were so helpful and friendly '},\n",
              "  {'score': 3,\n",
              "   'term': 'Ed help us with a last minute move. He was responsive  reasonable and made sure all was delivered as planned and'},\n",
              "  {'score': 4,\n",
              "   'term': 'had to take apart a dresser to fit it in my new place  but put it back together in under ten minutes.'},\n",
              "  {'score': 5, 'term': 'I am very happy with the'},\n",
              "  {'score': 6,\n",
              "   'term': 'the clothes separately. They moved each box and piece of furniture to the room specified  including a king Tempurpedic mattress upstairs (which is incredibly bulky'},\n",
              "  {'score': 7,\n",
              "   'term': 'treadmill into the basement. They easily disassembled the love seat and sofa backs to make navigating through the tight door spaces relatively simple. Always on'},\n",
              "  {'score': 8,\n",
              "   'term': 'asking me is there anything else here that you need to bring  were very aware of my three terrified cats hiding under the bed and'},\n",
              "  {'score': 9,\n",
              "   'term': 'and also moved many loose ends that we quickly threw in open boxes. They even took our three dressers  packed with clothes  as is  without'}],\n",
              " 'positive': [{'score': 0,\n",
              "   'term': 'local moving experience Made the move as painless as possible.'},\n",
              "  {'score': 2,\n",
              "   'term': 'reasonable and courteous. Great local moving experience Made the move as painless as possible.'},\n",
              "  {'score': 3,\n",
              "   'term': 'efficient  reasonable and courteous. Great local moving experience Made the move as painless as possible.'},\n",
              "  {'score': 4,\n",
              "   'term': 'a load of horror stories out there  and I REALLY didnt have the time or inclination to call dozens of companies. I asked my sister'},\n",
              "  {'score': 5,\n",
              "   'term': 'moving company to use  and there are a load of horror stories out there  and I REALLY didnt have the time or inclination to call'},\n",
              "  {'score': 6,\n",
              "   'term': 'I REALLY didnt have the time or inclination to call dozens of companies. I asked my sister if she had anyone shed recommend  and she'},\n",
              "  {'score': 7,\n",
              "   'term': 'and got all my stuff moved quickly and without damage. They had to take apart a dresser to fit it in my new place  but'},\n",
              "  {'score': 8,\n",
              "   'term': 'He was responsive  reasonable and made sure all was delivered as planned and without any hiccups.'},\n",
              "  {'score': 9, 'term': 'was delivered as planned and without any hiccups.'},\n",
              "  {'score': 11,\n",
              "   'term': 'my sister if she had anyone shed recommend  and she suggested I call Ed Skidmore. Hands down  they were THE best part of my entire'}]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN4eR2M5YOa6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}